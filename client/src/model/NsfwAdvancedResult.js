/**
 * imageapi
 * Image Recognition and Processing APIs let you use Artificial Intelligence and Machine Learning to recognize and process images, and also perform useful image modification operations.
 *
 * OpenAPI spec version: v1
 *
 * NOTE: This class is auto generated by the swagger code generator program.
 * https://github.com/swagger-api/swagger-codegen.git
 *
 * Swagger Codegen version: 2.3.1
 *
 * Do not edit the class manually.
 *
 */

(function(root, factory) {
  if (typeof define === 'function' && define.amd) {
    // AMD. Register as an anonymous module.
    define(['ApiClient'], factory);
  } else if (typeof module === 'object' && module.exports) {
    // CommonJS-like environments that support module.exports, like Node.
    module.exports = factory(require('../ApiClient'));
  } else {
    // Browser globals (root is window)
    if (!root.CloudmersiveImageApiClient) {
      root.CloudmersiveImageApiClient = {};
    }
    root.CloudmersiveImageApiClient.NsfwAdvancedResult = factory(root.CloudmersiveImageApiClient.ApiClient);
  }
}(this, function(ApiClient) {
  'use strict';




  /**
   * The NsfwAdvancedResult model module.
   * @module model/NsfwAdvancedResult
   * @version 1.4.0
   */

  /**
   * Constructs a new <code>NsfwAdvancedResult</code>.
   * Result of an Advanced NSFW classification
   * @alias module:model/NsfwAdvancedResult
   * @class
   */
  var exports = function() {
    var _this = this;













  };

  /**
   * Constructs a <code>NsfwAdvancedResult</code> from a plain JavaScript object, optionally creating a new instance.
   * Copies all relevant properties from <code>data</code> to <code>obj</code> if supplied or a new instance if not.
   * @param {Object} data The plain JavaScript object bearing properties of interest.
   * @param {module:model/NsfwAdvancedResult} obj Optional instance to populate.
   * @return {module:model/NsfwAdvancedResult} The populated <code>NsfwAdvancedResult</code> instance.
   */
  exports.constructFromObject = function(data, obj) {
    if (data) {
      obj = obj || new exports();

      if (data.hasOwnProperty('Successful')) {
        obj['Successful'] = ApiClient.convertToType(data['Successful'], 'Boolean');
      }
      if (data.hasOwnProperty('CleanResult')) {
        obj['CleanResult'] = ApiClient.convertToType(data['CleanResult'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsNudity')) {
        obj['ContainsNudity'] = ApiClient.convertToType(data['ContainsNudity'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsGraphicViolence')) {
        obj['ContainsGraphicViolence'] = ApiClient.convertToType(data['ContainsGraphicViolence'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsNonGraphicViolence')) {
        obj['ContainsNonGraphicViolence'] = ApiClient.convertToType(data['ContainsNonGraphicViolence'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsSelfHarm')) {
        obj['ContainsSelfHarm'] = ApiClient.convertToType(data['ContainsSelfHarm'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsHate')) {
        obj['ContainsHate'] = ApiClient.convertToType(data['ContainsHate'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsPotentialIllegalActivity')) {
        obj['ContainsPotentialIllegalActivity'] = ApiClient.convertToType(data['ContainsPotentialIllegalActivity'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsMedicalImagery')) {
        obj['ContainsMedicalImagery'] = ApiClient.convertToType(data['ContainsMedicalImagery'], 'Boolean');
      }
      if (data.hasOwnProperty('ContainsProfanity')) {
        obj['ContainsProfanity'] = ApiClient.convertToType(data['ContainsProfanity'], 'Boolean');
      }
      if (data.hasOwnProperty('Score')) {
        obj['Score'] = ApiClient.convertToType(data['Score'], 'Number');
      }
      if (data.hasOwnProperty('ClassificationOutcome')) {
        obj['ClassificationOutcome'] = ApiClient.convertToType(data['ClassificationOutcome'], 'String');
      }
    }
    return obj;
  }

  /**
   * True if the classification was successfully run, false otherwise
   * @member {Boolean} Successful
   */
  exports.prototype['Successful'] = undefined;
  /**
   * True if the result was clean, false otherwise
   * @member {Boolean} CleanResult
   */
  exports.prototype['CleanResult'] = undefined;
  /**
   * True if the image contains nudity or sex, false otherwise
   * @member {Boolean} ContainsNudity
   */
  exports.prototype['ContainsNudity'] = undefined;
  /**
   * True if the image contains graphic violence and/or gore, false otherwise
   * @member {Boolean} ContainsGraphicViolence
   */
  exports.prototype['ContainsGraphicViolence'] = undefined;
  /**
   * True if the image contains non-graphic violence, e.g. weapons, false otherwise
   * @member {Boolean} ContainsNonGraphicViolence
   */
  exports.prototype['ContainsNonGraphicViolence'] = undefined;
  /**
   * True if the image contains self-harm or suicide imagery, false otherwise
   * @member {Boolean} ContainsSelfHarm
   */
  exports.prototype['ContainsSelfHarm'] = undefined;
  /**
   * True if the image contains hate, false otherwise
   * @member {Boolean} ContainsHate
   */
  exports.prototype['ContainsHate'] = undefined;
  /**
   * True if the image contains potentially illegal activity such as drugs, false otherwise
   * @member {Boolean} ContainsPotentialIllegalActivity
   */
  exports.prototype['ContainsPotentialIllegalActivity'] = undefined;
  /**
   * True if the image contains medical imagery, false otherwise
   * @member {Boolean} ContainsMedicalImagery
   */
  exports.prototype['ContainsMedicalImagery'] = undefined;
  /**
   * True if the image contains profanity or obscenities, false otherwise
   * @member {Boolean} ContainsProfanity
   */
  exports.prototype['ContainsProfanity'] = undefined;
  /**
   * Score between 0.0 and 1.0.  Scores of 0.0-0.2 represent high probability safe content, while scores 0.8-1.0 represent high probability unsafe content.  Content between 0.2 and 0.8 is of increasing raciness.
   * @member {Number} Score
   */
  exports.prototype['Score'] = undefined;
  /**
   * Classification result into four categories: SafeContent_HighProbability, UnsafeContent_HighProbability, RacyContent, SafeContent_ModerateProbability
   * @member {String} ClassificationOutcome
   */
  exports.prototype['ClassificationOutcome'] = undefined;



  return exports;
}));


